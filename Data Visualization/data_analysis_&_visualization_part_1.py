# -*- coding: utf-8 -*-
"""Data Analysis & Visualization - Part 1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19QQib7RbFYAJz-MZAfqr3dsriniJxy6Y
"""

import pandas as pd
# Laoding the dataset
data=pd.read_csv('train.tsv',sep='\t')
data.head()

data.shape

# Identifying null values
data.isnull().sum()

import numpy as np
import pandas as pd
from os import path
from PIL import Image
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator
import matplotlib.pyplot as plt
import seaborn as sns

# Plotting word cloud for item description
from matplotlib.pyplot import figure
figure(figsize=(20, 20))
wordcloud = WordCloud(background_color="white").generate(text)

# Display the generated image:
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.savefig('item_dsecription.png')
plt.show()

# List of brands
brands=[str(brands).lower() for brands in data["brand_name"].unique()]
brands

import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
stop=stopwords.words('english')

# Extracting missing brand names from item "name" column
def extract_brand(name):
    name_tokens=name.lower().split()
    for n in name_tokens:
        if n in brands and n not in stop:
            return n
    return False
    
imputed_brands=[]
for index, row in data.iterrows():
    if str(row.brand_name)=="nan":
        brand=extract_brand(str(row[1]))
        if brand: 
            imputed_brands.append(brand)
        else: imputed_brands.append("missing")
    else:
        imputed_brands.append(row.brand_name.lower())

data["updated_brand_name"]=imputed_brands

# Data with updated brand name column
data

# Analyzing price variable distribution
print("Maximum Price: {}".format(max(data["price"])))
print("Minimum Price: {}".format(min(data["price"])))
print("Mean Price: {}".format(np.mean(data["price"])))
print("Median Price: {}".format(np.median(data["price"])))

fig, ax = plt.subplots(figsize=(10, 10))
sns.distplot(data['price'],)
ax.set_xlabel("Price in USD", fontsize = 15)
ax.set_ylabel("Density (frequency)", fontsize = 15)
plt.xlim(-25,1000)
ax.set_title("Distribution of price", fontsize=20)
plt.savefig("Skewed Price density.png")

# Creating log scaled price column
fig,ax = plt.subplots(figsize=(10, 10))
sns.distplot(data['updated_price'],)
ax.set_xlabel("log(Price in USD)", fontsize=15)
ax.set_ylabel("Density (frequency)",fontsize=15)
ax.set_title("Distribution of log(price)",fontsize=20)
plt.savefig("log(Price) density.png")

data

# Splitting category columns into different sub categories
data[["first_level_category","second_level_category","third_level_category","0","1"]]=data['category_name'].str.split('/', expand=True)
data=data.drop(columns=["category_name","0","1"])
data.head()



# Analyzing price vs item condition X category
means = data.groupby(['item_condition_id','first_level_category'])['price'].median()
means_df=pd.DataFrame(means)
heatmap=np.asarray(means_df["price"]).reshape(5,10)

heatmap

x_axis_labels = ["Beauty","Electronics","Handmade","Home","Kids","Men","Other","Sports & Outdoors","Vintage & Collectibels","Women"] # labels for x-axis
y_axis_labels = [1,2,3,4,5] # labels for y-axis

fig, ax = plt.subplots(figsize=(15, 7))
sns.heatmap(heatmap,xticklabels=x_axis_labels, yticklabels=y_axis_labels,annot=True,cmap="YlGnBu",cbar_kws={'label': 'Median Price'})
ax.set_xlabel("General Category", fontsize = 15)
ax.set_ylabel("Item Condition (1 being best 5 being worst)", fontsize = 15)
ax.set_title("Category x Condition and impact on price ", fontsize=20)
plt.savefig("Heatmap.png")


# create seabvorn heatmap with required labels
#sns.heatmap(flights_df, )



counts = data.groupby(['item_condition_id','first_level_category'])['price'].count()
count_df=pd.DataFrame(counts)
heatmap_count=np.asarray(count_df["price"]).reshape(5,10)

fig, ax = plt.subplots(figsize=(15, 7))
heatmap_count_normalize = heatmap_count/heatmap_count.max(axis=1, keepdims=True)
sns.heatmap(heatmap_count_normalize,xticklabels=x_axis_labels, yticklabels=y_axis_labels,cmap="YlGnBu",cbar_kws={'label': 'Proportion'})
ax.set_xlabel("General Category", fontsize = 1)
ax.set_ylabel("Item Condition (1 being best 5 being worst)", fontsize = 15)
ax.set_title("Proportion of categories falling under each item_condition ", fontsize=20)
plt.savefig("Heatmap2.png")

# Analyzing condition x shipping vs price
shipping_category = data.groupby(['shipping','item_condition_id'])['price'].median()
x_axis_labels = [1,2,3,4,5] # labels for x-axis

shipping_df=pd.DataFrame(shipping_category)
shipping_count=np.asarray(shipping_df["price"]).reshape(5,2)

fig, ax = plt.subplots(figsize=(15, 7))
#heatmap_count_count = shipping_count/heatmap_count.max(axis=1, keepdims=True)
sns.heatmap(shipping_count,xticklabels=["Shipping paid by buyer","Shipping paid by seller"], yticklabels=x_axis_labels,annot=True,cmap="YlGnBu",cbar_kws={'label': 'Median Price'})
ax.set_xlabel("General Category", fontsize = 1)
ax.set_ylabel("Item Condition (1 being best 5 being worst)", fontsize = 15)
ax.set_title("Proportion of categories falling under each item_condition ", fontsize=20)
plt.savefig("Heatmap_shipping.png")

shipping_category = data.groupby(['shipping','item_condition_id'])['price'].count()
x_axis_labels = [1,2,3,4,5] # labels for x-axis

shipping_df=pd.DataFrame(shipping_category)
shipping_count=np.asarray(shipping_df["price"]).reshape(5,2)

fig, ax = plt.subplots(figsize=(5, 5))
shipping_normalize = shipping_count/shipping_count.max(axis=0, keepdims=True)
sns.heatmap(shipping_normalize,xticklabels=["Shipping paid by buyer","Shipping paid by seller"], yticklabels=x_axis_labels,cmap="YlGnBu",cbar_kws={'label': 'Count density'})
#ax.set_xlabel("General Category", fontsize = 1)
ax.set_ylabel("Item Condition (1 being best 5 being worst)", fontsize = 10)
ax.set_title(" Count density of items falling under each item_condition vs shipping ", fontsize=12)
plt.savefig("Heatmap_shipping.png")

# Splitting dataset into dev:test (9:1)
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(data.drop(columns=['price']), data["price"], test_size=0.10, random_state=42)

X_train,X_test=train_test_split(data, test_size=0.10, random_state=42)

len(X_test)

# Writing X_train, X_test into csvs
X_train.to_csv('train.csv', index=False)
X_test.to_csv('test.csv', index=False)

